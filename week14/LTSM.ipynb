{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190756dd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391f09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d40ff",
   "metadata": {},
   "source": [
    "# Similar to CNN and reads in data and seperates from the h5 file and turn it into a ds structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81330a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 1000.0 Hz\n",
      "State: NREM\n",
      "Segment IDs: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "State: WAKE\n",
      "Segment IDs: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '4', '5', '6', '7', '8', '9']\n",
      "Signals shape: (55, 285000)\n",
      "Labels shape: (55,)\n"
     ]
    }
   ],
   "source": [
    "h5_path = 'Part1SubjectHB10.h5'\n",
    "\n",
    "f = h5py.File(h5_path, 'r')\n",
    "\n",
    "fs = f.attrs['fs'][0]\n",
    "print(\"Sampling rate: %.1f Hz\" % (fs))\n",
    "\n",
    "states = []\n",
    "for name, grp in f.items():\n",
    "    states.append(name)\n",
    "    print(\"State: %s\" % (name))\n",
    "    print(\"Segment IDs:\", list(grp.keys()))\n",
    "\n",
    "lfp = {key: [] for key in states}\n",
    "for key in states:\n",
    "    group = f[key]\n",
    "    n = len(group)\n",
    "    for i in range(n):\n",
    "        lfp[key].append(group[str(i+1)][()].astype(float))\n",
    "\n",
    "all_signals = lfp['NREM'] + lfp['WAKE']\n",
    "all_labels = [0] * len(lfp['NREM']) + [1] * len(lfp['WAKE'])\n",
    "\n",
    "max_length = max(signal.shape[0] for signal in all_signals)\n",
    "\n",
    "padded_signals = [np.pad(signal, (0, max_length - signal.shape[0]), mode='constant') if signal.shape[0] < max_length else signal[:max_length] for signal in all_signals]\n",
    "\n",
    "# Stack the padded signals\n",
    "signals = np.stack(padded_signals)\n",
    "labels = np.array(all_labels)\n",
    "\n",
    "print(\"Signals shape:\", signals.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79a87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5SignalDataset(Dataset):\n",
    "    def __init__(self, signals, labels):\n",
    "        self.data = torch.tensor(signals, dtype=torch.float32)\n",
    "        if len(self.data.shape) == 2:\n",
    "            self.data = self.data.unsqueeze(-1)  # (N, Length, 1) for LSTM\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "full_dataset = H5SignalDataset(signals, labels)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a148b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.6973, Val Loss: 0.6953, Train Acc: 0.3864, Val Acc: 0.4545\n"
     ]
    }
   ],
   "source": [
    "class SignalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=2, num_layers=2):\n",
    "        super(SignalLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # output of the classification\n",
    "        return out\n",
    "\n",
    "model = SignalLSTM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for signals, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in val_loader:\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c241890",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60633ef",
   "metadata": {},
   "source": [
    "LSTM is a tupe of classification method for TNN and it is dependent on gateways during training. This LSTM model processes the raw signal one timestep at a time and outputs a hidden representation to classify the layers. The training is similar to the last CNN model and on the forward pass it calculates and predicts per epoch and then on the back updates the weights based on the error calculations, training is used to minimize training loss from the data and allows it to better accurately predict using the raw signal as inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
