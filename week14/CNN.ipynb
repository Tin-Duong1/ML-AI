{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ca2dbb",
   "metadata": {},
   "source": [
    "# CNN classification\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1decb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99832c",
   "metadata": {},
   "source": [
    "# Define file path and read h5 data provided in project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f300ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 1000.0 Hz\n",
      "State: NREM\n",
      "Segment IDs: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "State: WAKE\n",
      "Segment IDs: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '4', '5', '6', '7', '8', '9']\n",
      "Signals shape: (55, 285000)\n",
      "Labels shape: (55,)\n"
     ]
    }
   ],
   "source": [
    "h5_path = 'Part1SubjectHB10.h5' # File path to the h5 file that contains the data to process\n",
    "\n",
    "f = h5py.File(h5_path, 'r') # using the library allows h5 to be read in as a dictionary\n",
    "\n",
    "# Read the sampling frequency\n",
    "fs = f.attrs['fs'][0]\n",
    "print(\"Sampling rate: %.1f Hz\" % (fs))\n",
    "\n",
    "# Read states\n",
    "states = []\n",
    "for name, grp in f.items():\n",
    "    states.append(name)\n",
    "    print(\"State: %s\" % (name))\n",
    "    print(\"Segment IDs:\", list(grp.keys()))\n",
    "\n",
    "# Extract LFP segments\n",
    "lfp = {key: [] for key in states}\n",
    "for key in states:\n",
    "    group = f[key]\n",
    "    n = len(group)\n",
    "    for i in range(n):\n",
    "        lfp[key].append(group[str(i+1)][()].astype(float))\n",
    "\n",
    "# Combine signals and labels\n",
    "all_signals = lfp['NREM'] + lfp['WAKE']\n",
    "all_labels = [0] * len(lfp['NREM']) + [1] * len(lfp['WAKE'])\n",
    "\n",
    "# Find the maximum length of the signals\n",
    "max_length = max(signal.shape[0] for signal in all_signals)\n",
    "\n",
    "# Pad or trunc all signals to the same length\n",
    "padded_signals = [np.pad(signal, (0, max_length - signal.shape[0]), mode='constant') if signal.shape[0] < max_length else signal[:max_length] for signal in all_signals]\n",
    "\n",
    "# Stack the signals and convert labels to a NumPy array\n",
    "signals = np.stack(padded_signals)\n",
    "labels = np.array(all_labels)\n",
    "\n",
    "print(\"Signals shape:\", signals.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1cf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5SignalDataset(Dataset):\n",
    "    def __init__(self, signals, labels):\n",
    "        self.data = torch.tensor(signals, dtype=torch.float32)\n",
    "        if len(self.data.shape) == 2:\n",
    "            self.data = self.data.unsqueeze(1)  # (N, 1, Length) if necessary\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): # method to return length of dataset data passed in\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): # method to return the data and label at the index passed in\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "full_dataset = H5SignalDataset(signals, labels) # create the ds obj\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset)) # 80% of the data for training\n",
    "val_size = len(full_dataset) - train_size # 20% of the data for validation\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size]) # split the dataset into training and validation sets\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # create the data loader for the training set\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # create the data loader for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d21920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 73126.2297, Val Loss: 38434.8008, Train Acc: 0.5227, Val Acc: 0.1818\n",
      "Epoch 2/5, Train Loss: 23603.7729, Val Loss: 12546.4160, Train Acc: 0.4091, Val Acc: 0.8182\n",
      "Epoch 3/5, Train Loss: 9898.1824, Val Loss: 10854.3281, Train Acc: 0.6591, Val Acc: 0.8182\n",
      "Epoch 4/5, Train Loss: 3588.3537, Val Loss: 8240.7061, Train Acc: 0.7500, Val Acc: 0.8182\n",
      "Epoch 5/5, Train Loss: 2037.0518, Val Loss: 6004.2559, Train Acc: 0.8409, Val Acc: 0.8182\n"
     ]
    }
   ],
   "source": [
    "class SignalCNN(nn.Module):\n",
    "    def __init__(self, input_length): # constructor for the CNN model\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        feature_length = input_length // 2 // 2\n",
    "        self.fc1 = nn.Linear(32 * feature_length, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x): #Forward pass through the model to calculate the output using RELU activation function\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "if len(signals.shape) == 3:\n",
    "    input_length = signals.shape[2]\n",
    "else:\n",
    "    input_length = signals.shape[1]\n",
    "\n",
    "model = SignalCNN(input_length) # instantiate the model \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # optimizer\n",
    "\n",
    "epochs = 5 # number of iterations for training the model\n",
    "train_losses = [] \n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs): # loop through the number of epoches to train the model and create the loss and accuracy scores\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for signals, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in val_loader:\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38b514",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aef919",
   "metadata": {},
   "source": [
    "The model performs well as the accuracy and training loss goes down which means it is generalizing well and not overfitting to the dataset. The input for the model is the raw data that was extracted from project 3 and used for training the model through 5 epoches. The CNN model using pytorch learns features from this dataset and is able to generate and predict accurately using the loss and validation set. During training it uses filters to find out patterns in the data and predict the new outcome. The forward pass allows the model to predict and adapt and the backwards allows it to change its weights if the error is way off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
